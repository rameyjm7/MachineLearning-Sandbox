{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ab054-2597-4726-9018-0daa023151e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c76bdb-0a94-44e0-807d-45576b6e3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Load dataset function\n",
    "def load_data(selected_classes, unknown_class=True):\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    y_train = y_train.flatten()\n",
    "    y_test = y_test.flatten()\n",
    "\n",
    "    # Filter known classes\n",
    "    mask_train = np.isin(y_train, selected_classes)\n",
    "    mask_test = np.isin(y_test, selected_classes)\n",
    "\n",
    "    x_train_known, y_train_known = x_train[mask_train], y_train[mask_train]\n",
    "    x_test_known, y_test_known = x_test[mask_test], y_test[mask_test]\n",
    "\n",
    "    # Re-map labels to 0-N range\n",
    "    label_map = {old_label: new_label for new_label, old_label in enumerate(selected_classes)}\n",
    "    y_train_known = np.vectorize(label_map.get)(y_train_known)\n",
    "    y_test_known = np.vectorize(label_map.get)(y_test_known)\n",
    "\n",
    "    # Normalize images\n",
    "    x_train_known, x_test_known = x_train_known / 255.0, x_test_known / 255.0\n",
    "\n",
    "    # Add \"Unknown\" class (samples from other classes)\n",
    "    if unknown_class:\n",
    "        unknown_mask_train = ~np.isin(y_train, selected_classes)\n",
    "        unknown_mask_test = ~np.isin(y_test, selected_classes)\n",
    "\n",
    "        x_unknown_train, y_unknown_train = x_train[unknown_mask_train], np.full(np.sum(unknown_mask_train), len(selected_classes))\n",
    "        x_unknown_test, y_unknown_test = x_test[unknown_mask_test], np.full(np.sum(unknown_mask_test), len(selected_classes))\n",
    "\n",
    "        # Limit the number of unknown samples added\n",
    "        max_new_unknowns = 3000\n",
    "        x_unknown_train, y_unknown_train = x_unknown_train[:max_new_unknowns], y_unknown_train[:max_new_unknowns]\n",
    "        x_unknown_test, y_unknown_test = x_unknown_test[:max_new_unknowns], y_unknown_test[:max_new_unknowns]\n",
    "\n",
    "        x_train = np.concatenate([x_train_known, x_unknown_train], axis=0)\n",
    "        y_train = np.concatenate([y_train_known, y_unknown_train], axis=0)\n",
    "        x_test = np.concatenate([x_test_known, x_unknown_test], axis=0)\n",
    "        y_test = np.concatenate([y_test_known, y_unknown_test], axis=0)\n",
    "    else:\n",
    "        x_train, y_train, x_test, y_test = x_train_known, y_train_known, x_test_known, y_test_known\n",
    "\n",
    "    # One-hot encode labels\n",
    "    num_classes = len(selected_classes) + (1 if unknown_class else 0)\n",
    "    y_train, y_test = to_categorical(y_train, num_classes), to_categorical(y_test, num_classes)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, num_classes, label_map\n",
    "\n",
    "# Detect unknown samples\n",
    "def detect_unknown_samples(model, x_test, threshold=0.05, entropy_threshold=0.6):\n",
    "    predictions = model.predict(x_test)\n",
    "    \n",
    "    # Extract classification output\n",
    "    if isinstance(predictions, tuple) or isinstance(predictions, list):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    max_probs = np.max(predictions, axis=1)\n",
    "    pred_entropy = entropy(predictions.T)\n",
    "\n",
    "    # Detect unknowns: Low confidence OR high entropy\n",
    "    unknown_indices = np.where((max_probs < threshold) | (pred_entropy > entropy_threshold))[0]\n",
    "    return unknown_indices\n",
    "\n",
    "# Update model to handle new class\n",
    "def update_model(model, num_classes):\n",
    "    x = model.layers[-2].output\n",
    "    new_output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    new_model = Model(inputs=model.input, outputs=new_output)\n",
    "    return new_model\n",
    "\n",
    "# Train model and detect unknown classes dynamically\n",
    "def train_and_detect_unknowns():\n",
    "    known_classes = [0, 1, 2, 3, 4]\n",
    "    detected_classes = set(known_classes)\n",
    "    histories = {}\n",
    "    confusion_matrices = {}\n",
    "\n",
    "    x_train, y_train, x_test, y_test, num_classes, label_map = load_data(known_classes)\n",
    "\n",
    "    # Initialize ResNet model\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\", name=\"classification_output\")(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train initial model\n",
    "    print(\"\\n--- Initial Training on Known Classes + Unknown ---\")\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=64, verbose=1)\n",
    "    histories[\"Initial\"] = history.history\n",
    "\n",
    "    # Introduce new classes dynamically\n",
    "    for new_class in sorted(range(5, 10)):  \n",
    "        print(f\"\\n--- Checking for Unknown Samples: Potential Class {new_class} ---\")\n",
    "        \n",
    "        unknown_indices = detect_unknown_samples(model, x_test)\n",
    "        print(f\"Detected {len(unknown_indices)} unknown samples for potential class {new_class}.\")\n",
    "\n",
    "        if len(unknown_indices) > 50:  # Add new class if threshold met\n",
    "            print(f\"Threshold met! Adding new class {new_class}.\")\n",
    "            detected_classes.add(new_class)\n",
    "            x_train, y_train, x_test, y_test, num_classes, label_map = load_data(list(detected_classes))\n",
    "\n",
    "            # Update model with new class\n",
    "            model = update_model(model, num_classes)\n",
    "            model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "            # Train with the new class\n",
    "            epochs_per_class = {5: 30, 6: 30, 7: 30, 8: 40, 9: 50}\n",
    "            history = model.fit(x_train, y_train, validation_data=(x_test, y_test), \n",
    "                                epochs=epochs_per_class[new_class], batch_size=64, verbose=1)\n",
    "            histories[f\"Class_{new_class}\"] = history.history\n",
    "\n",
    "            # Compute confusion matrix for this class\n",
    "            y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "            y_true = np.argmax(y_test, axis=1)\n",
    "            cm = confusion_matrix(y_true, y_pred, labels=np.arange(num_classes))\n",
    "            confusion_matrices[f\"Class_{new_class}\"] = cm\n",
    "\n",
    "    return histories, confusion_matrices, list(detected_classes), model, x_test, y_test\n",
    "\n",
    "# Train model and get final results\n",
    "histories, confusion_matrices, final_classes, trained_model, final_x_test, final_y_test = train_and_detect_unknowns()\n",
    "\n",
    "# Plot per-class confusion matrices\n",
    "for label, cm in confusion_matrices.items():\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f'Confusion Matrix - {label}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "# Compute and plot final confusion matrix for all classes\n",
    "final_predictions = np.argmax(trained_model.predict(final_x_test), axis=1)\n",
    "final_true_labels = np.argmax(final_y_test, axis=1)\n",
    "final_cm = confusion_matrix(final_true_labels, final_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(final_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Final Confusion Matrix - All Classes\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb15c2-bfbf-44a4-8c34-9d7cbfaa3ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Replay Buffer for Balancing Data\n",
    "# -------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=3000):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add_samples(self, x, y):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer = self.buffer[len(y):]  # Remove old samples\n",
    "        self.buffer.extend(zip(x, y))\n",
    "\n",
    "    def get_samples(self, batch_size=500):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        x_replay, y_replay = zip(*samples)\n",
    "        return np.array(x_replay), np.array(y_replay)\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Load CIFAR-10 Data\n",
    "# -------------------------\n",
    "def load_data(selected_classes, unknown_class=True):\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    y_train = y_train.flatten()\n",
    "    y_test = y_test.flatten()\n",
    "\n",
    "    # Filter known classes\n",
    "    mask_train = np.isin(y_train, selected_classes)\n",
    "    mask_test = np.isin(y_test, selected_classes)\n",
    "\n",
    "    x_train_known, y_train_known = x_train[mask_train], y_train[mask_train]\n",
    "    x_test_known, y_test_known = x_test[mask_test], y_test[mask_test]\n",
    "\n",
    "    # Re-map labels to 0-N range\n",
    "    label_map = {old_label: new_label for new_label, old_label in enumerate(selected_classes)}\n",
    "    y_train_known = np.vectorize(label_map.get)(y_train_known)\n",
    "    y_test_known = np.vectorize(label_map.get)(y_test_known)\n",
    "\n",
    "    # Normalize images\n",
    "    x_train_known, x_test_known = x_train_known / 255.0, x_test_known / 255.0\n",
    "\n",
    "    # Add \"Unknown\" class\n",
    "    if unknown_class:\n",
    "        unknown_mask_train = ~np.isin(y_train, selected_classes)\n",
    "        unknown_mask_test = ~np.isin(y_test, selected_classes)\n",
    "\n",
    "        x_unknown_train, y_unknown_train = x_train[unknown_mask_train], np.full(np.sum(unknown_mask_train), len(selected_classes))\n",
    "        x_unknown_test, y_unknown_test = x_test[unknown_mask_test], np.full(np.sum(unknown_mask_test), len(selected_classes))\n",
    "\n",
    "        # Limit the number of unknown samples added\n",
    "        max_new_unknowns = 3000\n",
    "        x_unknown_train, y_unknown_train = x_unknown_train[:max_new_unknowns], y_unknown_train[:max_new_unknowns]\n",
    "        x_unknown_test, y_unknown_test = x_unknown_test[:max_new_unknowns], y_unknown_test[:max_new_unknowns]\n",
    "\n",
    "        x_train = np.concatenate([x_train_known, x_unknown_train], axis=0)\n",
    "        y_train = np.concatenate([y_train_known, y_unknown_train], axis=0)\n",
    "        x_test = np.concatenate([x_test_known, x_unknown_test], axis=0)\n",
    "        y_test = np.concatenate([y_test_known, y_unknown_test], axis=0)\n",
    "    else:\n",
    "        x_train, y_train, x_test, y_test = x_train_known, y_train_known, x_test_known, y_test_known\n",
    "\n",
    "    # One-hot encode labels\n",
    "    num_classes = len(selected_classes) + (1 if unknown_class else 0)\n",
    "    y_train, y_test = to_categorical(y_train, num_classes), to_categorical(y_test, num_classes)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, num_classes, label_map\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Detect Unknown Samples\n",
    "# -------------------------\n",
    "def detect_unknown_samples(model, x_test, threshold=0.1, entropy_threshold=0.5):\n",
    "    predictions = model.predict(x_test)\n",
    "    if isinstance(predictions, tuple) or isinstance(predictions, list):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    max_probs = np.max(predictions, axis=1)\n",
    "    pred_entropy = entropy(predictions.T)\n",
    "\n",
    "    # Dynamic thresholding\n",
    "    softmax_dynamic_threshold = np.percentile(max_probs, 10)\n",
    "    entropy_dynamic_threshold = np.percentile(pred_entropy, 90)\n",
    "\n",
    "    # Detect unknowns\n",
    "    unknown_indices = np.where(\n",
    "        (max_probs < max(threshold, softmax_dynamic_threshold)) |\n",
    "        (pred_entropy > min(entropy_threshold, entropy_dynamic_threshold))\n",
    "    )[0]\n",
    "    return unknown_indices\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Expand Model for New Class\n",
    "# -------------------------\n",
    "def expand_model(model, num_classes):\n",
    "    for layer in model.layers[:-2]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = model.layers[-2].output\n",
    "    new_output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    new_model = Model(inputs=model.input, outputs=new_output)\n",
    "\n",
    "    return new_model\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Train Model and Detect Unknown Classes\n",
    "# -------------------------\n",
    "def train_and_detect_unknowns():\n",
    "    known_classes = [0, 1, 2, 3, 4]\n",
    "    detected_classes = set(known_classes)\n",
    "    histories = {}\n",
    "    confusion_matrices = {}\n",
    "\n",
    "    x_train, y_train, x_test, y_test, num_classes, label_map = load_data(known_classes)\n",
    "    replay_buffer = ReplayBuffer()\n",
    "\n",
    "    # Initialize ResNet model\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Initial training\n",
    "    print(\"\\n--- Initial Training on Known Classes + Unknown ---\")\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=64, verbose=1)\n",
    "    histories[\"Initial\"] = history.history\n",
    "\n",
    "    for new_class in range(5, 10):\n",
    "        print(f\"\\n--- Checking for Unknown Samples: Potential Class {new_class} ---\")\n",
    "\n",
    "        unknown_indices = detect_unknown_samples(model, x_test)\n",
    "        print(f\"Detected {len(unknown_indices)} unknown samples for class {new_class}.\")\n",
    "\n",
    "        if len(unknown_indices) > 50:\n",
    "            print(f\"Threshold met! Adding new class {new_class}.\")\n",
    "            detected_classes.add(new_class)\n",
    "            x_train, y_train, x_test, y_test, num_classes, label_map = load_data(list(detected_classes))\n",
    "\n",
    "            # Update model\n",
    "            model = expand_model(model, num_classes)\n",
    "            model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "            # Train model with new class\n",
    "            history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=30, batch_size=64, verbose=1)\n",
    "            histories[f\"Class_{new_class}\"] = history.history\n",
    "\n",
    "    return histories, model, x_test, y_test\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Train & Evaluate Model\n",
    "# -------------------------\n",
    "histories, trained_model, final_x_test, final_y_test = train_and_detect_unknowns()\n",
    "\n",
    "# Compute final confusion matrix\n",
    "final_predictions = np.argmax(trained_model.predict(final_x_test), axis=1)\n",
    "final_true_labels = np.argmax(final_y_test, axis=1)\n",
    "final_cm = confusion_matrix(final_true_labels, final_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(final_cm, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "plt.title(\"Final Confusion Matrix - All Classes\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d2be13-838a-4bb0-aec9-f1093fdbda08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8d588-d563-4b9a-b1c4-6b8eafeabc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Replay Buffer for Balancing Data\n",
    "# -------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=3000):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add_samples(self, x, y):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer = self.buffer[len(y):]  # Remove old samples\n",
    "        self.buffer.extend(zip(x, y))\n",
    "\n",
    "    def get_samples(self, batch_size=500):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        x_replay, y_replay = zip(*samples)\n",
    "        return np.array(x_replay), np.array(y_replay)\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Load CIFAR-10 Data\n",
    "# -------------------------\n",
    "def load_data(selected_classes, unknown_class=True):\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    y_train = y_train.flatten()\n",
    "    y_test = y_test.flatten()\n",
    "\n",
    "    # Filter known classes\n",
    "    mask_train = np.isin(y_train, selected_classes)\n",
    "    mask_test = np.isin(y_test, selected_classes)\n",
    "\n",
    "    x_train_known, y_train_known = x_train[mask_train], y_train[mask_train]\n",
    "    x_test_known, y_test_known = x_test[mask_test], y_test[mask_test]\n",
    "\n",
    "    # Re-map labels to 0-N range\n",
    "    label_map = {old_label: new_label for new_label, old_label in enumerate(selected_classes)}\n",
    "    y_train_known = np.vectorize(label_map.get)(y_train_known)\n",
    "    y_test_known = np.vectorize(label_map.get)(y_test_known)\n",
    "\n",
    "    # Normalize images\n",
    "    x_train_known, x_test_known = x_train_known / 255.0, x_test_known / 255.0\n",
    "\n",
    "    # Add \"Unknown\" class\n",
    "    if unknown_class:\n",
    "        unknown_mask_train = ~np.isin(y_train, selected_classes)\n",
    "        unknown_mask_test = ~np.isin(y_test, selected_classes)\n",
    "\n",
    "        x_unknown_train, y_unknown_train = x_train[unknown_mask_train], np.full(np.sum(unknown_mask_train), len(selected_classes))\n",
    "        x_unknown_test, y_unknown_test = x_test[unknown_mask_test], np.full(np.sum(unknown_mask_test), len(selected_classes))\n",
    "\n",
    "        # Limit the number of unknown samples added\n",
    "        max_new_unknowns = 3000\n",
    "        x_unknown_train, y_unknown_train = x_unknown_train[:max_new_unknowns], y_unknown_train[:max_new_unknowns]\n",
    "        x_unknown_test, y_unknown_test = x_unknown_test[:max_new_unknowns], y_unknown_test[:max_new_unknowns]\n",
    "\n",
    "        x_train = np.concatenate([x_train_known, x_unknown_train], axis=0)\n",
    "        y_train = np.concatenate([y_train_known, y_unknown_train], axis=0)\n",
    "        x_test = np.concatenate([x_test_known, x_unknown_test], axis=0)\n",
    "        y_test = np.concatenate([y_test_known, y_unknown_test], axis=0)\n",
    "    else:\n",
    "        x_train, y_train, x_test, y_test = x_train_known, y_train_known, x_test_known, y_test_known\n",
    "\n",
    "    # One-hot encode labels\n",
    "    num_classes = len(selected_classes) + (1 if unknown_class else 0)\n",
    "    y_train, y_test = to_categorical(y_train, num_classes), to_categorical(y_test, num_classes)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, num_classes, label_map\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Improve Model Fine-Tuning (Replay Buffer & Knowledge Distillation)\n",
    "# -------------------------\n",
    "def fine_tune_model(trained_model, x_train, y_train, x_test, y_test):\n",
    "    # Learning rate decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-4,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    trained_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    print(\"\\n--- Fine-Tuning Model with All Classes ---\")\n",
    "    history_finetune = trained_model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "                                         epochs=30, batch_size=64, verbose=1)\n",
    "    return history_finetune\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Final Training Setup\n",
    "# -------------------------\n",
    "def train_final_model():\n",
    "    final_classes = list(range(10))  # All CIFAR-10 classes\n",
    "    x_train, y_train, x_test, y_test, num_classes, label_map = load_data(final_classes, unknown_class=False)\n",
    "\n",
    "    # Initialize ResNet model\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train the final model\n",
    "    print(\"\\n--- Initial Training on All Classes ---\")\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=30, batch_size=64, verbose=1)\n",
    "\n",
    "    return model, x_test, y_test\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Train and Evaluate Final Model\n",
    "# -------------------------\n",
    "trained_model, final_x_test, final_y_test = train_final_model()\n",
    "\n",
    "# Fine-tune the model further\n",
    "fine_tune_model(trained_model, final_x_test, final_y_test, final_x_test, final_y_test)\n",
    "\n",
    "# Compute final confusion matrix\n",
    "final_predictions = np.argmax(trained_model.predict(final_x_test), axis=1)\n",
    "final_true_labels = np.argmax(final_y_test, axis=1)\n",
    "final_cm = confusion_matrix(final_true_labels, final_predictions)\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Plot Normalized Confusion Matrix\n",
    "# -------------------------\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Final Normalized Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(final_cm, list(range(10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f6f7d-8028-4698-89c0-66fd5e9cb560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CIFAR-10 class labels\n",
    "cifar10_labels = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Select random samples\n",
    "num_samples = 10  # Number of images to display\n",
    "indices = random.sample(range(len(final_x_test)), num_samples)\n",
    "sample_images = final_x_test[indices]\n",
    "true_labels = np.argmax(final_y_test[indices], axis=1)\n",
    "predicted_labels = np.argmax(trained_model.predict(sample_images), axis=1)\n",
    "\n",
    "# Plot images with predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = sample_images[i]\n",
    "    true_label = cifar10_labels[true_labels[i]]\n",
    "    predicted_label = cifar10_labels[predicted_labels[i]]\n",
    "\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"True: {true_label}\\nPred: {predicted_label}\", fontsize=10)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76579a97-8225-40bd-82ca-de08a4e9dbda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
