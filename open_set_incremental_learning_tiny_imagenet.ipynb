{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30fb15c2-bfbf-44a4-8c34-9d7cbfaa3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 21:08:12.094800: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-25 21:08:12.105954: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742951292.118803 3160641 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742951292.122652 3160641 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-25 21:08:12.135767: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1742951297.461208 3160641 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1077 MB memory:  -> device: 0, name: NVIDIA A30, pci bus id: 0000:e1:00.0, compute capability: 8.0\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__RealDiv_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:RealDiv] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 103\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, x_val, y_pred, y_true, cm\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# ðŸ”¹ Run Training & Evaluation\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m trained_model, val_images, val_preds, val_labels, final_cm \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# ðŸ”¹ Plot Normalized Confusion Matrix for Tiny ImageNet\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_confusion_matrix\u001b[39m(cm, labels):\n",
      "Cell \u001b[0;32mIn[1], line 85\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_evaluate\u001b[39m():\n\u001b[0;32m---> 85\u001b[0m     x_train, y_train, x_val, y_val, num_classes \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     model \u001b[38;5;241m=\u001b[39m build_model(num_classes)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training on Tiny ImageNet ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m img \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mdecode_jpeg(img, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Decode the image\u001b[39;00m\n\u001b[1;32m     32\u001b[0m img \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mresize(img, [\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m])  \u001b[38;5;66;03m# Resize to 64x64\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m  \u001b[38;5;66;03m# Normalize the image\u001b[39;00m\n\u001b[1;32m     34\u001b[0m x_train\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m     35\u001b[0m y_train\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[0;32m/apps/easybuild/software/falcon-sapphirerapids/Python/3.11.5-GCCcore-13.2.0/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/apps/easybuild/software/falcon-sapphirerapids/Python/3.11.5-GCCcore-13.2.0/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__RealDiv_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:RealDiv] name: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Load Tiny ImageNet Data using TensorFlow\n",
    "# -------------------------\n",
    "def load_data():\n",
    "    # Directories for Tiny ImageNet\n",
    "    train_dir = 'tiny-imagenet-200/train/'\n",
    "    val_dir = 'tiny-imagenet-200/val/'\n",
    "    \n",
    "    # Load the training images and labels\n",
    "    x_train, y_train = [], []\n",
    "    class_names = os.listdir(train_dir)\n",
    "    class_names.sort()  # Sorting to ensure class order remains the same\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(train_dir, class_name, 'images')\n",
    "        for image_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, image_name)\n",
    "            img = tf.io.read_file(img_path)\n",
    "            img = tf.image.decode_jpeg(img, channels=3)  # Decode the image\n",
    "            img = tf.image.resize(img, [64, 64])  # Resize to 64x64\n",
    "            img = img / 255.0  # Normalize the image\n",
    "            x_train.append(img)\n",
    "            y_train.append(i)\n",
    "    \n",
    "    x_train = np.array(x_train, dtype=np.float32)\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = to_categorical(y_train, 200)  # One-hot encode labels\n",
    "\n",
    "    # Load the validation images and labels\n",
    "    x_val, y_val = [], []\n",
    "    val_images = os.listdir(val_dir)\n",
    "    with open(os.path.join(val_dir, 'val_annotations.txt'), 'r') as f:\n",
    "        val_info = f.readlines()\n",
    "    \n",
    "    for line in val_info:\n",
    "        image_name, class_name = line.strip().split('\\t')[:2]\n",
    "        img_path = os.path.join(val_dir, 'images', image_name)\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)  # Decode the image\n",
    "        img = tf.image.resize(img, [64, 64])  # Resize to 64x64\n",
    "        img = img / 255.0  # Normalize the image\n",
    "        x_val.append(img)\n",
    "        y_val.append(class_names.index(class_name))\n",
    "    \n",
    "    x_val = np.array(x_val, dtype=np.float32)\n",
    "    y_val = np.array(y_val)\n",
    "    y_val = to_categorical(y_val, 200)  # One-hot encode labels\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, 200\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Build Model (ResNet50)\n",
    "# -------------------------\n",
    "def build_model(num_classes):\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "    # Custom layers\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Train Model\n",
    "# -------------------------\n",
    "def train_and_evaluate():\n",
    "    x_train, y_train, x_val, y_val, num_classes = load_data()\n",
    "    model = build_model(num_classes)\n",
    "\n",
    "    print(\"\\n--- Training on Tiny ImageNet ---\")\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=20, batch_size=64, verbose=1)\n",
    "\n",
    "    print(\"\\n--- Evaluating on Full Dataset ---\")\n",
    "    y_pred = np.argmax(model.predict(x_val), axis=1)\n",
    "    y_true = np.argmax(y_val, axis=1)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return model, x_val, y_pred, y_true, cm\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Run Training & Evaluation\n",
    "# -------------------------\n",
    "trained_model, val_images, val_preds, val_labels, final_cm = train_and_evaluate()\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Plot Normalized Confusion Matrix for Tiny ImageNet\n",
    "# -------------------------\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_normalized, annot=False, fmt=\".2f\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Final Normalized Confusion Matrix - Tiny ImageNet\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "\n",
    "# Labels for Tiny ImageNet classes\n",
    "tiny_imagenet_labels = list(range(200))  # 200 classes\n",
    "plot_confusion_matrix(final_cm, tiny_imagenet_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d8d588-d563-4b9a-b1c4-6b8eafeabc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Replay Buffer for Balancing Data\n",
    "# -------------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=5000):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add_samples(self, x, y):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer = self.buffer[len(y):]  # Remove old samples\n",
    "        self.buffer.extend(zip(x, y))\n",
    "\n",
    "    def get_samples(self, batch_size=1000):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        x_replay, y_replay = zip(*samples)\n",
    "        return np.array(x_replay), np.array(y_replay)\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Load Tiny ImageNet Data using TensorFlow\n",
    "# -------------------------\n",
    "def load_data():\n",
    "    # Directories for Tiny ImageNet\n",
    "    train_dir = 'tiny-imagenet-200/train/'\n",
    "    val_dir = 'tiny-imagenet-200/val/'\n",
    "    \n",
    "    # Load the training images and labels\n",
    "    x_train, y_train = [], []\n",
    "    class_names = os.listdir(train_dir)\n",
    "    class_names.sort()  # Sorting to ensure class order remains the same\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(train_dir, class_name, 'images')\n",
    "        for image_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, image_name)\n",
    "            img = tf.io.read_file(img_path)\n",
    "            img = tf.image.decode_jpeg(img, channels=3)  # Decode the image\n",
    "            img = tf.image.resize(img, [64, 64])  # Resize to 64x64\n",
    "            img = img / 255.0  # Normalize the image\n",
    "            x_train.append(img)\n",
    "            y_train.append(i)\n",
    "    \n",
    "    x_train = np.array(x_train, dtype=np.float32)\n",
    "    y_train = np.array(y_train)\n",
    "    y_train = to_categorical(y_train, 200)  # One-hot encode labels\n",
    "\n",
    "    # Load the validation images and labels\n",
    "    x_val, y_val = [], []\n",
    "    val_images = os.listdir(val_dir)\n",
    "    with open(os.path.join(val_dir, 'val_annotations.txt'), 'r') as f:\n",
    "        val_info = f.readlines()\n",
    "    \n",
    "    for line in val_info:\n",
    "        image_name, class_name = line.strip().split('\\t')[:2]\n",
    "        img_path = os.path.join(val_dir, 'images', image_name)\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)  # Decode the image\n",
    "        img = tf.image.resize(img, [64, 64])  # Resize to 64x64\n",
    "        img = img / 255.0  # Normalize the image\n",
    "        x_val.append(img)\n",
    "        y_val.append(class_names.index(class_name))\n",
    "    \n",
    "    x_val = np.array(x_val, dtype=np.float32)\n",
    "    y_val = np.array(y_val)\n",
    "    y_val = to_categorical(y_val, 200)  # One-hot encode labels\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, 200\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Improve Model Fine-Tuning (Replay Buffer & Knowledge Distillation)\n",
    "# -------------------------\n",
    "def fine_tune_model(trained_model, x_train, y_train, x_test, y_test):\n",
    "    # Learning rate decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-4,\n",
    "        decay_steps=20000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    trained_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    print(\"\\n--- Fine-Tuning Model on Tiny ImageNet ---\")\n",
    "    history_finetune = trained_model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "                                         epochs=50, batch_size=64, verbose=1)\n",
    "    return history_finetune\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Final Training Setup\n",
    "# -------------------------\n",
    "def train_final_model():\n",
    "    x_train, y_train, x_test, y_test, num_classes = load_data()\n",
    "\n",
    "    # Initialize ResNet model\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(64, 64, 3))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train the final model\n",
    "    print(\"\\n--- Initial Training on Tiny ImageNet ---\")\n",
    "    history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=64, verbose=1)\n",
    "\n",
    "    return model, x_test, y_test\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Train and Evaluate Final Model\n",
    "# -------------------------\n",
    "trained_model, final_x_test, final_y_test = train_final_model()\n",
    "\n",
    "# Fine-tune the model further\n",
    "fine_tune_model(trained_model, final_x_test, final_y_test, final_x_test, final_y_test)\n",
    "\n",
    "# Compute final confusion matrix\n",
    "final_predictions = np.argmax(trained_model.predict(final_x_test), axis=1)\n",
    "final_true_labels = np.argmax(final_y_test, axis=1)\n",
    "final_cm = confusion_matrix(final_true_labels, final_predictions)\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Plot Normalized Confusion Matrix\n",
    "# -------------------------\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_normalized, annot=False, fmt=\".2f\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(\"Final Normalized Confusion Matrix (Tiny ImageNet)\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(final_cm, list(range(200)))  # 200 classes for Tiny ImageNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f6f7d-8028-4698-89c0-66fd5e9cb560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Save the Trained Model\n",
    "# -------------------------\n",
    "model_filename = \"tiny_imagenet_resnet50_model.h5\"\n",
    "trained_model.save(model_filename)\n",
    "print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Load Tiny ImageNet Labels (Human Readable)\n",
    "# -------------------------\n",
    "tiny_imagenet_label_names = [\n",
    "    'n02118413_bangalore_terrier', 'n02119789_kit_fox', 'n02120505_fox_squirrel', 'n02127052_lynx', 'n02128385_leopard',\n",
    "    'n02129165_snow_leopard', 'n02129604_tiger_cat', 'n02130308_cheetah', 'n02132136_black_footed_ferret', 'n02134084_ice_fish',\n",
    "    # ... you need to include all 200 class names here\n",
    "    # Add the rest of the 200 class names from Tiny ImageNet\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# ðŸ”¹ Display 50 Random Predictions with True Labels\n",
    "# -------------------------\n",
    "num_samples = 50\n",
    "indices = random.sample(range(len(test_images)), num_samples)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, idx in enumerate(indices):\n",
    "    image = test_images[idx]\n",
    "    true_label = tiny_imagenet_label_names[test_labels[idx]]\n",
    "    predicted_label = tiny_imagenet_label_names[test_preds[idx]]\n",
    "\n",
    "    plt.subplot(10, 5, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"True: {true_label}\\nPred: {predicted_label}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630552cb-0656-42d4-81d5-090b1c744cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
