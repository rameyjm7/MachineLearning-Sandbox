{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180598f0-f806-418b-9671-730426978bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:47:48.283303: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-27 15:47:48.295830: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745783268.308989 3125256 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745783268.312879 3125256 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-27 15:47:48.327323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1745783273.247020 3125256 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22280 MB memory:  -> device: 0, name: NVIDIA A30, pci bus id: 0000:4a:00.0, compute capability: 8.0\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745783274.412537 3125256 service.cc:148] XLA service 0x16bb33a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745783274.412556 3125256 service.cc:156]   StreamExecutor device (0): NVIDIA A30, Compute Capability 8.0\n",
      "2025-04-27 15:47:54.433333: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1745783274.533174 3125256 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-04-27 15:47:54.682622: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "I0000 00:00:1745783276.043425 3125256 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Cell 1: Train & save GAN generator\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D, BatchNormalization, LeakyReLU, Flatten, Dense,\n",
    "    Embedding, Concatenate, UpSampling1D, Activation, Reshape, Dropout\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "\n",
    "# 1) Load & prepare IQ data\n",
    "with open(\"RML2016.10a_dict.pkl\", \"rb\") as f:\n",
    "    raw = pickle.load(f, encoding=\"latin1\")\n",
    "\n",
    "X, y, snr = [], [], []\n",
    "for (mod, S), sigs in raw.items():\n",
    "    for sig in sigs:\n",
    "        X.append(np.vstack([sig[0], sig[1]]).T)  # (128,2)\n",
    "        y.append(mod)\n",
    "        snr.append(S)\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y)\n",
    "snr = np.array(snr)\n",
    "\n",
    "# 2) Encode labels and split high‐SNR for GAN training\n",
    "le = LabelEncoder(); y_enc = le.fit_transform(y)\n",
    "mask = snr > 6\n",
    "X_train = X[mask]; y_train = y_enc[mask]\n",
    "n_classes = len(le.classes_)\n",
    "\n",
    "# 3) GAN definitions\n",
    "def define_discriminator(input_shape, n_classes):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Conv1D(32,3,padding='same',kernel_initializer=init)(inp)\n",
    "    x = LeakyReLU(0.2)(x); x = Dropout(0.5)(x)\n",
    "    x = Conv1D(64,3,strides=2,padding='same',kernel_initializer=init)(x)\n",
    "    x = BatchNormalization()(x); x = LeakyReLU(0.2)(x); x = Dropout(0.5)(x)\n",
    "    x = Conv1D(128,3,strides=2,padding='same',kernel_initializer=init)(x)\n",
    "    x = BatchNormalization()(x); x = LeakyReLU(0.2)(x); x = Dropout(0.5)(x)\n",
    "    x = Flatten()(x)\n",
    "    out_real  = Dense(1, activation='sigmoid')(x)\n",
    "    out_class = Dense(n_classes, activation='softmax')(x)\n",
    "    model = Model(inp, [out_real, out_class])\n",
    "    model.compile(optimizer=Adam(1e-4, beta_1=0.5),\n",
    "                  loss=['binary_crossentropy','sparse_categorical_crossentropy'])\n",
    "    return model\n",
    "\n",
    "def define_generator(latent_dim, n_classes):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    in_lbl   = Input(shape=(1,))\n",
    "    li       = Embedding(n_classes, 50)(in_lbl)\n",
    "    li       = Dense(latent_dim, kernel_initializer=init)(li)\n",
    "    li       = Reshape((latent_dim,))(li)\n",
    "    in_noise = Input(shape=(latent_dim,))\n",
    "    merge    = Concatenate()([in_noise, li])\n",
    "    x        = Dense(64*32, kernel_initializer=init)(merge)\n",
    "    x        = Activation('relu')(x)\n",
    "    x        = Reshape((32,64))(x)\n",
    "    x        = UpSampling1D()(x)\n",
    "    x        = Conv1D(64,5,padding='same',kernel_initializer=init)(x)\n",
    "    x        = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "    x        = UpSampling1D()(x)\n",
    "    x        = Conv1D(32,5,padding='same',kernel_initializer=init)(x)\n",
    "    x        = BatchNormalization()(x); x = Activation('relu')(x)\n",
    "    x        = Conv1D(2,5,padding='same',kernel_initializer=init)(x)\n",
    "    out      = Activation('tanh')(x)\n",
    "    return Model([in_noise, in_lbl], out)\n",
    "\n",
    "def define_gan(gen, disc):\n",
    "    for layer in disc.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    out = disc(gen.output)\n",
    "    m   = Model(gen.input, out)\n",
    "    m.compile(optimizer=Adam(1e-4, beta_1=0.5),\n",
    "              loss=['binary_crossentropy','sparse_categorical_crossentropy'])\n",
    "    return m\n",
    "\n",
    "# 4) Training utilities\n",
    "def gen_latent(latent_dim, n, n_classes):\n",
    "    return np.random.randn(n, latent_dim), np.random.randint(0, n_classes, n)\n",
    "\n",
    "def gen_real(X, y, n):\n",
    "    idx = np.random.randint(0, len(X), n)\n",
    "    return [X[idx], y[idx]], np.ones((n,1))\n",
    "\n",
    "def gen_fake(gen, latent_dim, n, n_classes):\n",
    "    z, labels = gen_latent(latent_dim, n, n_classes)\n",
    "    Xf = gen.predict([z, labels], verbose=0)\n",
    "    return [Xf, labels], np.zeros((n,1))\n",
    "\n",
    "# 5) Build & train GAN\n",
    "latent_dim = 100; epochs_gan = 100; bs = 64\n",
    "disc = define_discriminator((128,2), n_classes)\n",
    "gen  = define_generator(latent_dim, n_classes)\n",
    "gan  = define_gan(gen, disc)\n",
    "\n",
    "half_bs = bs//2\n",
    "steps   = max(1, len(X_train)//bs)\n",
    "for _ in range(epochs_gan):\n",
    "    for _ in range(steps):\n",
    "        (Xr, yr), _   = gen_real(X_train, y_train, half_bs)\n",
    "        disc.train_on_batch(Xr, [np.ones((half_bs,1)), yr])\n",
    "        (Xf, yf), _   = gen_fake(gen, latent_dim, half_bs, n_classes)\n",
    "        disc.train_on_batch(Xf, [np.zeros((half_bs,1)), yf])\n",
    "        z, zl         = gen_latent(latent_dim, bs, n_classes)\n",
    "        gan.train_on_batch([z, zl], [np.ones((bs,1)), zl])\n",
    "\n",
    "# 6) Save generator for later\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "gen.save(\"saved_models/gan_generator.keras\")\n",
    "print(\"✅ GAN generator saved to saved_models/gan_generator.keras\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f2287-7aab-4c87-96f4-e0b419de959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Cell 2: Load GAN, augment data, retrain & compare -- verbose training\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 1) Reload IQ data & labels\n",
    "with open(\"RML2016.10a_dict.pkl\",\"rb\") as f:\n",
    "    raw = pickle.load(f, encoding=\"latin1\")\n",
    "\n",
    "X, y, snr = [], [], []\n",
    "for (mod, S), sigs in raw.items():\n",
    "    for sig in sigs:\n",
    "        X.append(np.vstack([sig[0], sig[1]]).T)\n",
    "        y.append(mod)\n",
    "        snr.append(S)\n",
    "X   = np.array(X, dtype=np.float32)\n",
    "y   = np.array(y)\n",
    "snr = np.array(snr)\n",
    "\n",
    "# 2) Encode and split\n",
    "le            = LabelEncoder(); y_enc = le.fit_transform(y)\n",
    "mask          = snr > 6\n",
    "X_train_real  = X[mask]; y_train_real = y_enc[mask]\n",
    "X_test, y_test= X, y_enc\n",
    "snr_test      = snr\n",
    "n_classes     = len(le.classes_)\n",
    "\n",
    "# 3) Load pre-trained GAN generator\n",
    "gen = load_model(\"saved_models/gan_generator.keras\")\n",
    "\n",
    "# 4) Synthesize same-size dataset\n",
    "latent_dim = gen.input[0].shape[-1]\n",
    "n_syn      = len(X_train_real)\n",
    "z, lbls    = np.random.randn(n_syn, latent_dim), np.random.randint(0, n_classes, n_syn)\n",
    "X_syn      = gen.predict([z, lbls], verbose=1)\n",
    "y_syn      = lbls\n",
    "\n",
    "# 5) Augment & shuffle\n",
    "X_aug = np.vstack([X_train_real, X_syn])\n",
    "y_aug = np.concatenate([y_train_real, y_syn])\n",
    "perm  = np.random.permutation(len(X_aug))\n",
    "X_aug, y_aug = X_aug[perm], y_aug[perm]\n",
    "\n",
    "# 6) Classifier definition\n",
    "def make_clf(input_shape, n_classes):\n",
    "    m = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.5),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    m.compile(optimizer=Adam(1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return m\n",
    "\n",
    "# 7) Baseline (real-only)\n",
    "baseline = make_clf((128,2), n_classes)\n",
    "baseline.fit( X_train_real, y_train_real,\n",
    "              epochs=50, batch_size=64, verbose=1 )\n",
    "y_base = baseline.predict(X_test, verbose=1).argmax(axis=1)\n",
    "acc_base = accuracy_score(y_test, y_base)\n",
    "\n",
    "# 8) GAN-augmented\n",
    "aug = make_clf((128,2), n_classes)\n",
    "aug.fit( X_aug, y_aug,\n",
    "         epochs=50, batch_size=64, verbose=1 )\n",
    "y_aug = aug.predict(X_test, verbose=1).argmax(axis=1)\n",
    "acc_aug = accuracy_score(y_test, y_aug)\n",
    "\n",
    "print(f\"\\nBaseline acc: {acc_base:.4f}    Augmented acc: {acc_aug:.4f}\")\n",
    "\n",
    "# 9) Accuracy vs SNR plot\n",
    "snrs = np.unique(snr_test)\n",
    "acc_base_snr = [accuracy_score(y_test[snr_test==s], y_base[snr_test==s]) for s in snrs]\n",
    "acc_aug_snr  = [accuracy_score(y_test[snr_test==s], y_aug[snr_test==s])  for s in snrs]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(snrs, acc_base_snr, label='Baseline')\n",
    "plt.plot(snrs, acc_aug_snr,  label='Augmented')\n",
    "plt.xlabel(\"SNR [dB]\"); plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs SNR\")\n",
    "plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 10) Confusion matrices\n",
    "cm_b = confusion_matrix(y_test, y_base)\n",
    "cm_a = confusion_matrix(y_test, y_aug)\n",
    "fig, ax = plt.subplots(1,2,figsize=(14,6))\n",
    "sns.heatmap(cm_b, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_, ax=ax[0])\n",
    "ax[0].set_title(\"Baseline CM\")\n",
    "sns.heatmap(cm_a, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_, ax=ax[1])\n",
    "ax[1].set_title(\"Augmented CM\")\n",
    "for a in ax: a.set_xlabel(\"Pred\"); a.set_ylabel(\"True\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ddece-22ed-43ba-9338-304bb0e86fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Cell: Compare Original 2024 LSTM vs Baseline vs GAN‐Augmented\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming X_test, y_test, snr_test, le, baseline_clf, aug_clf exist in namespace\n",
    "# If not, reimport/define/preload them as in previous cells.\n",
    "\n",
    "# 1) Load original 2024 model\n",
    "orig_clf = load_model(\"lstm_rnn_2024.keras\")\n",
    "\n",
    "# 2) Predict and evaluate\n",
    "y_orig_pred = orig_clf.predict(X_test, verbose=0).argmax(axis=1)\n",
    "acc_orig     = accuracy_score(y_test, y_orig_pred)\n",
    "y_base_pred  = baseline_clf.predict(X_test, verbose=0).argmax(axis=1)\n",
    "acc_base     = accuracy_score(y_test, y_base_pred)\n",
    "y_aug_pred   = aug_clf.predict(X_test, verbose=0).argmax(axis=1)\n",
    "acc_aug      = accuracy_score(y_test, y_aug_pred)\n",
    "\n",
    "print(f\"Original 2024 model accuracy: {acc_orig:.4f}\")\n",
    "print(f\"Baseline (real‐only) accuracy: {acc_base:.4f}\")\n",
    "print(f\"GAN‐Augmented accuracy:      {acc_aug:.4f}\")\n",
    "\n",
    "# 3) Accuracy vs SNR for all three\n",
    "snr_vals = np.unique(snr_test)\n",
    "acc_orig_snr = [accuracy_score(y_test[snr_test==s], y_orig_pred[snr_test==s]) for s in snr_vals]\n",
    "acc_base_snr = [accuracy_score(y_test[snr_test==s], y_base_pred[snr_test==s]) for s in snr_vals]\n",
    "acc_aug_snr  = [accuracy_score(y_test[snr_test==s], y_aug_pred[snr_test==s])  for s in snr_vals]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(snr_vals, acc_orig_snr, label='Original 2024')\n",
    "plt.plot(snr_vals, acc_base_snr, label='Baseline')\n",
    "plt.plot(snr_vals, acc_aug_snr,  label='Augmented')\n",
    "plt.xlabel(\"SNR [dB]\"); plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs SNR: Original vs Baseline vs Augmented\")\n",
    "plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 4) Confusion matrices side by side\n",
    "cm_orig = confusion_matrix(y_test, y_orig_pred)\n",
    "cm_base = confusion_matrix(y_test, y_base_pred)\n",
    "cm_aug  = confusion_matrix(y_test, y_aug_pred)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18,6))\n",
    "for ax, cm, title in zip(\n",
    "    axs,\n",
    "    [cm_orig, cm_base, cm_aug],\n",
    "    [\"Original 2024\", \"Baseline\", \"GAN‐Augmented\"]\n",
    "):\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae487126-b463-48ad-b550-154de00e1915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
